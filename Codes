#libraries and loading data

import numpy as np 
import pandas as pd 
import os
import random
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import BatchNormalization, RandomFlip, RandomZoom, RandomRotation, Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, Dropout, Activation
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

test_path = '../input/histopathologic-cancer-detection/test/'
train_path = '../input/histopathologic-cancer-detection/train/'
train_data = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')

tf.__version__


# initial data inspection

train_data.info()
print("")
print(train_data.head())
print("")
print(train_data.describe())
print("")
print(len(os.listdir(test_path)))


sample_tumor_ids = train_data[train_data['label'] == 1]['id'].sample(n=3).values
sample_normal_ids = train_data[train_data['label'] == 0]['id'].sample(n=3).values

tumor_images = []
for id in sample_tumor_ids:
    image_path = os.path.join(train_path, id + '.tif')  
    tumor_images.append(Image.open(image_path))
    
normal_images = []
for id in sample_normal_ids:
    image_path = os.path.join(train_path, id + '.tif') 
    normal_images.append(Image.open(image_path))
    
plt.figure(figsize=(10, 3))
for i, image in enumerate(tumor_images):
    plt.subplot(1, 3, i+1)
    plt.imshow(image)
    plt.title('Tumor')
    plt.axis('off')
plt.show()

plt.figure(figsize=(10, 3))
for i, image in enumerate(normal_images):
    plt.subplot(1, 3, i+1)
    plt.imshow(image)
    plt.title('Normal')
    plt.axis('off')
plt.show()




# get count
label_counts = train_data['label'].value_counts()

# histogram
plt.figure(figsize=(8, 6))
ax = label_counts.plot(kind='bar')
plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(rotation=0)

# % label
total = len(train_data)
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height()/total)
    x = p.get_x() + p.get_width() / 2 - 0.05
    y = p.get_height() + total * 0.001
    ax.annotate(percentage, (x, y), ha='center')

plt.show()
train_data["id"] = train_data["id"].apply(lambda x: x + ".tif")
train_data["label"] = train_data["label"].astype(str)




datagen = ImageDataGenerator(rescale=1./255.,
                            validation_split=0.2)





train_generator = datagen.flow_from_dataframe(
    dataframe=train_data,
    directory=train_path,
    x_col="id",
    y_col="label",
    subset="training",
    batch_size=256,
    seed=13,
    class_mode="binary",
    target_size=(64,64),
    shuffle=True)



valid_generator = datagen.flow_from_dataframe(
    dataframe=train_data,
    directory=train_path,
    x_col="id",
    y_col="label",
    subset="validation",
    batch_size=256,
    seed=13,
    class_mode="binary",
    target_size=(64,64),
    shuffle=True)


model1_ROC = tf.keras.metrics.AUC()

model1 = Sequential()
    
model1.add(Conv2D(filters=16, kernel_size=(3,3)))
model1.add(Conv2D(filters=16, kernel_size=(3,3)))
model1.add(MaxPooling2D(pool_size=(2,2)))

model1.add(Conv2D(filters=32, kernel_size=(3,3)))
model1.add(Conv2D(filters=32, kernel_size=(3,3)))
model1.add(AveragePooling2D(pool_size=(2,2)))

model1.add(Flatten())
model1.add(Dense(1, activation='sigmoid'))
    
model1.build(input_shape=(32, 64, 64, 3))
    
model1.compile(loss='binary_crossentropy', metrics=['accuracy', model1_ROC])
    
model1.summary()



his_model1 = model1.fit(
                        train_generator,
                        steps_per_epoch=687,
                        epochs = 5,
                        validation_data = valid_generator,
                        validation_steps=171,
                        verbose=1)




acc = his_model1.history['accuracy']
val_acc = his_model1.history['val_accuracy']
loss = his_model1.history['loss']
val_loss = his_model1.history['val_loss']
roc_auc = his_model1.history['auc']
val_roc_auc = his_model1.history['val_auc']

epochs = range(len(acc))

# accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 3, 1)
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

# loss
plt.subplot(1, 3, 2)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()

# ROC AUC
plt.subplot(1, 3, 3)
plt.plot(epochs, roc_auc, label='Training ROC AUC')
plt.plot(epochs, val_roc_auc, label='Validation ROC AUC')
plt.title('Training and Validation ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()
model2_ROC = tf.keras.metrics.AUC()

model2 = Sequential()

model2.add(Conv2D(filters=16, kernel_size=(3,3)))
model2.add(BatchNormalization())
model2.add(Activation("relu"))
model2.add(Conv2D(filters=16, kernel_size=(3,3)))
model2.add(BatchNormalization())
model2.add(Activation("relu"))
model2.add(MaxPooling2D(pool_size=(2,2)))
model2.add(Dropout(0.1))

model2.add(Conv2D(filters=32, kernel_size=(3,3)))
model2.add(BatchNormalization())
model2.add(Activation("relu"))
model2.add(Conv2D(filters=32, kernel_size=(3,3)))
model2.add(BatchNormalization())
model2.add(Activation("relu"))
model2.add(AveragePooling2D(pool_size=(2,2)))
model2.add(Dropout(0.1))

model2.add(Flatten())
model2.add(Dense(1, activation = "sigmoid"))

model2.build(input_shape=(32, 64, 64, 3))

model2.compile(Adam(0.001), loss = "binary_crossentropy", metrics=['accuracy', model2_ROC])

model2.summary()






his_model2 = model2.fit(
                        train_generator, 
                        steps_per_epoch=687, 
                        epochs=5,
                        validation_data=valid_generator,
                        validation_steps=171,
                        verbose=1)


print(his_model2.history.keys())





acc2 = his_model2.history['accuracy']
val_acc2 = his_model2.history['val_accuracy']
loss2 = his_model2.history['loss']
val_loss2 = his_model2.history['val_loss']
roc_auc2 = his_model2.history['auc_2']
val_roc_auc2 = his_model2.history['val_auc_2']

epochs = range(len(acc))

# accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 3, 1)
plt.plot(epochs, acc2, label='Training Accuracy')
plt.plot(epochs, val_acc2, label='Validation Accuracy')
plt.title('Model 2: Training and Validation Accuracy')
plt.legend()

# loss
plt.subplot(1, 3, 2)
plt.plot(epochs, loss2, label='Training Loss')
plt.plot(epochs, val_loss2, label='Validation Loss')
plt.title('Model 2: Training and Validation Loss')
plt.legend()

# ROC AUC
plt.subplot(1, 3, 3)
plt.plot(epochs, roc_auc2, label='Training ROC AUC')
plt.plot(epochs, val_roc_auc2, label='Validation ROC AUC')
plt.title('Model 2: Training and Validation ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

train_generator_2 = datagen.flow_from_dataframe(
    dataframe=train_data,
    directory=train_path,
    x_col="id",
    y_col="label",
    subset="training",
    batch_size=64,
    seed=13,
    class_mode="binary",
    target_size=(64,64),
    shuffle=True)
valid_generator2 = datagen.flow_from_dataframe(
    dataframe=train_data,
    directory=train_path,
    x_col="id",
    y_col="label",
    subset="validation",
    batch_size=64,
    seed=13,
    class_mode="binary",
    target_size=(64,64),
    shuffle=True)

from keras.regularizers import l2

model3_ROC = tf.keras.metrics.AUC()

model3 = Sequential()

reg = l2(0.001)  

model3.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', kernel_regularizer=reg))
model3.add(BatchNormalization())
model3.add(Activation("relu"))
model3.add(Conv2D(filters=16, kernel_size=(3,3), kernel_regularizer=reg))
model3.add(BatchNormalization())
model3.add(Activation("relu"))
model3.add(MaxPooling2D(pool_size=(2,2)))
model3.add(Dropout(0.3))

model3.add(Conv2D(filters=32, kernel_size=(3,3), kernel_regularizer=reg))
model3.add(BatchNormalization())
model3.add(Activation("relu"))
model3.add(Conv2D(filters=32, kernel_size=(3,3), kernel_regularizer=reg))
model3.add(BatchNormalization())
model3.add(Activation("relu"))
model3.add(AveragePooling2D(pool_size=(2,2)))
model3.add(Dropout(0.3))

model3.add(Flatten())
model3.add(Dense(1, activation="sigmoid", kernel_regularizer=reg))

model3.build(input_shape=(32, 64, 64, 3))

model3.compile(Adam(learning_rate=0.0001), loss="binary_crossentropy", metrics=['accuracy', model2_ROC])

model3.summary()

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)
reduce = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)
his_model3 = model3.fit(
                    train_generator_2, 
                    steps_per_epoch=2750, 
                    validation_data=valid_generator2,
                    validation_steps=687,
                    epochs=5,
                    callbacks=[reduce, earlystopper])
print(his_model3.history.keys())


acc3 = his_model3.history['accuracy']
val_acc3 = his_model3.history['val_accuracy']
loss3 = his_model3.history['loss']
val_loss3 = his_model3.history['val_loss']
roc_auc3 = his_model3.history['auc_2']
val_roc_auc3 = his_model3.history['val_auc_2']

epochs = range(1,4)

# accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 3, 1)
plt.plot(epochs, acc3, label='Training Accuracy')
plt.plot(epochs, val_acc3, label='Validation Accuracy')
plt.title('Model 3: Training and Validation Accuracy')
plt.legend()

# loss
plt.subplot(1, 3, 2)
plt.plot(epochs, loss3, label='Training Loss')
plt.plot(epochs, val_loss3, label='Validation Loss')
plt.title('Model 3: Training and Validation Loss')
plt.legend()

# ROC AUC
plt.subplot(1, 3, 3)
plt.plot(epochs, roc_auc3, label='Training ROC AUC')
plt.plot(epochs, val_roc_auc3, label='Validation ROC AUC')
plt.title('Model 3: Training and Validation ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()
test_data = pd.DataFrame({'id':os.listdir(test_path)})
test_data.head()
datagen_test = ImageDataGenerator(rescale=1./255.)

test_generator = datagen_test.flow_from_dataframe(
    dataframe=test_data,
    directory=test_path,
    x_col='id', 
    y_col=None,
    target_size=(64,64),
    batch_size=1,
    shuffle=False,
    class_mode=None)
results = model3.predict(test_generator, verbose=1)
results = np.transpose(results)[0]
submission_df = pd.DataFrame()
submission_df['id'] = test_data['id'].apply(lambda x: x.split('.')[0])
submission_df['label'] = list(map(lambda x: 0 if x < 0.5 else 1, results))
submission_df.head()
submission_df.to_csv('submission.csv', index=False)
